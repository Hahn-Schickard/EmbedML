{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelle auf Embedded Geräten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook ist Teil des Projekts [EmbedML](https://hahn-schickard.gitbook.io/embedml) und basiert auf den Inhalten des Kapitels [Modelle auf Embedded Geräten](https://hahn-schickard.gitbook.io/embedml/3_tinyml/3.4_modelle_auf_embedded_geraeten). Es ist auf [Colab](https://colab.research.google.com/drive/1gqcQ54VTYV4zW6mC6MQMqHC-x4etYXDF?usp=sharing) und im [GitHub-Repository von Hahn-Schickard](https://github.com/Hahn-Schickard/EmbedML/blob/master/src/Modelle%20auf%20embedded%20Ger%C3%A4ten.ipynb) zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation der benötigten Bibliotheken in den angegebenen Versionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Das System kann den angegebenen Pfad nicht finden.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy==1.26.4 tensorflow-gpu==2.10 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden Ordner erstellt, falls diese noch nicht vohanden sind, in welchen die in diesem Notebook erstellten Modelle gespeichert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('assets')):\n",
    "    os.makedirs(os.path.join('assets'))\n",
    "    \n",
    "if not os.path.exists(os.path.join('assets', 'models')):\n",
    "    os.makedirs(os.path.join('assets', 'models'))\n",
    "    \n",
    "if not os.path.exists(os.path.join('assets', 'models', 'c++')):\n",
    "    os.makedirs(os.path.join('assets', 'models', 'c++'))\n",
    "    \n",
    "if not os.path.exists(os.path.join('assets', 'models', 'tf')):\n",
    "    os.makedirs(os.path.join('assets', 'models', 'tf'))\n",
    "    \n",
    "if not os.path.exists(os.path.join('assets', 'models', 'tflite')):\n",
    "    os.makedirs(os.path.join('assets', 'models', 'tflite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Basline Modell wird trainiert, welches als Grundlage für das Notebook dient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt, wird der MNIST-Datensatz geladen. Die Pixelwerte des MNIST-Datensatzes werden durch 255 geteilt, um sie auf den Bereich von 0 bis 1 zu normalisieren, da die ursprünglichen Werte zwischen 0 und 255 liegen. Im selben Schritt werden die Daten, in den Datentyp float32 konvertiert und in Trainings- und Testdaten aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train / 255.0).astype(np.float32)\n",
    "X_test = (X_test / 255.0).astype(np.float32)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Verwendung von np.expand_dims mit axis=3 wird eine neue Dimension an der vierten Position (Index 3) der Arrays `X_train` und `X_test` hinzugefügt. Durch das Hinzufügen einer zusätzlichen Dimension wird die Form zu (Anzahl_Bilder, Höhe, Breite, Kanäle), was später für das neuronale Netz erforderlich ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `create_model` definiert ein sequentielles neuronales Netzwerk in TensorFlow, das für die Klassifikation von Bildern mit einer Eingabegröße von 28x28x1 Pixeln konzipiert ist. Das Modell beginnt mit einer 2D-Faltungsschicht (Conv2D), um Merkmale aus den Eingabedaten zu extrahieren, gefolgt von einer Max-Pooling-Schicht (MaxPooling2D), die die räumlichen Dimensionen der Daten reduziert. Diese Schichten folgen erneut. Die resultierenden Merkmalskarten werden durch eine Flatten-Schicht in einen eindimensionalen Vektor umgewandelt. Dieser Vektor wird dann durch vollvernetzte Schichten (Dense) verarbeitet, gefolgt von Dropout-Schichten (Dropout) zur Vermeidung von Überanpassung. Abschließend liefert eine Dense-Ausgabeschicht mit 10 Neuronen und der Softmax-Aktivierungsfunktion die Wahrscheinlichkeiten für jede der 10 Klassen. Das Modell wird mit dem Adam-Optimizer trainiert und verwendet die Verlustfunktion sparse_categorical_crossentropy. Als Metrik zur Bewertung der Modellleistung wird die Genauigkeit (accuracy) herangezogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Definition eines neuronalen Netzes\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird das zuvor definierte Modell mit den Trainingsdaten `X_train` und den zugehörigen Label `y_train`, beispielhaft für 5 Epochen trainiert, wobei 20% der Daten für die Validierung verwendet werden, um die Leistung des Modells während des Trainings zu überwachen und Überanpassung zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                12832     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,010\n",
      "Trainable params: 16,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 20s 9ms/step - loss: 0.9521 - accuracy: 0.6764 - val_loss: 0.2126 - val_accuracy: 0.9425\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.3768 - accuracy: 0.8828 - val_loss: 0.1329 - val_accuracy: 0.9632\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.2890 - accuracy: 0.9130 - val_loss: 0.1106 - val_accuracy: 0.9681\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.2406 - accuracy: 0.9280 - val_loss: 0.0947 - val_accuracy: 0.9734\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.2126 - accuracy: 0.9363 - val_loss: 0.0864 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2915225ff10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ursprüngliche Modell wird mit den Testdaten evaluiert, um die Testgenauigkeit zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Baseline Test Accuracy: {baseline_model_accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das trainierte TensorFlow Modell wird in die Datei `mnist_model.h5` gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('assets', 'models', 'tf', 'mnist_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellkonvertierung zu TensorFlow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `representative_dataset_gen` dient dazu, eine repräsentative Stichprobe des Trainingsdatensatzes für die Kalibrierung eines TensorFlow Lite Modells bereitzustellen. Sie iteriert hier über die ersten 500 Datenpunkte des Trainingsdatensatzes `X_train` und gibt jedes Beispiel als Liste mit einem Element zurück. Dies ist erforderlich, da das Modell nur einen Eingabewert erwartet, und die Kalibrierungsfunktion eine Liste von Eingabewerten benötigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen(dataset):\n",
    "    # Erstellen eines tf.data.Dataset und Batch-Größe von 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(1)\n",
    "    # Iteriere über die ersten 500 Beispiele des Trainingsdatensatzes\n",
    "    for input_value in dataset.take(500):\n",
    "        # Das Modell erwartet nur einen Eingabewert, daher wird jedes Beispiel\n",
    "        # als Liste mit einem einzigen Element zurückgegeben.\n",
    "        yield [input_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Laden des TensorFlow-Modells und erstellen eines `TFLiteConverter`-Objekts aus diesem Modell. Der `TFLiteConverter` wird verwendet, um das TensorFlow-Modell in ein TensorFlow Lite Modell zu konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(os.path.join(\"assets\", \"models\", \"tf\", \"mnist_model.h5\"))\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Zelle werden die Parameter für die Quantisierung festgelegt. Die Variable `quantization` wird `True` gesetzt, um anzugeben, dass die Quantisierung während der Modellkonvertierung durchgeführt werden soll. Außerdem wird `quant_dtype` auf `int8 only` festgelegt, um zu spezifizieren, dass die Quantisierung ausschließlich auf `int8`-Basis erfolgen soll. Andernfalls (`None`) werden für Input- und Output-Werte der Datentyp float32 verwendet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization = True\n",
    "quant_dtype = \"int8 only\"  # None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird geprüft, ob die Quantisierung angewendet werden soll. Falls ja, werden die Optimierungen für den Konvertierungsprozess definiert:\n",
    "- Mit `tf.lite.Optimize.DEFAULT` wird die standardmäßige Optimierung aktiviert.\n",
    "- Wenn die Quantisierung auf int8-Basis durchgeführt werden soll (`int8 only` in `quant_dtype`), muss dem  Converter ein repräsentativer Datensatz zugewiesen werden. Dieser Datensatz wird benötigt, um den Wertebereich der Modellgewichte während der Quantisierung zu kalibrieren.\n",
    "- Die unterstützten Operationen werden auf TFLite-Ops festgelegt, um sicherzustellen, dass nur int8-Operationen verwendet werden.\n",
    "- Schließlich wird der Eingabe- und Ausgabetyp für Inferenzen auf `tf.uint8` festgelegt, um sicherzustellen, dass das Modell während der Inferenzen im int8-Format arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfung, ob Quantisierung angewendet werden soll\n",
    "if quantization:\n",
    "    # Optimierungen werden auf Standardwerte gesetzt, um die Konvertierung zu optimieren\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Überprüfung, ob eine Quantisierung auf int8-Basis durchgeführt werden soll\n",
    "    if quant_dtype == \"int8 only\":\n",
    "        # Festlegen eines repräsentativen Datensatzes für die statische Quantisierung\n",
    "        converter.representative_dataset = lambda: representative_dataset_gen(X_train)\n",
    "        # Festlegen der unterstützten Operationen auf TensorFlow Lite eingebauten int8-Operationen\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        # Festlegen des Eingabetyps für Inferenzen auf 8-Bit Ganzzahlen (uint8)\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        # Festlegen des Ausgabetyps für Inferenzen auf 8-Bit Ganzzahlen (uint8)\n",
    "        converter.inference_output_type = tf.uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das TensorFlow-Modell wird in ein TensorFlow Lite-Modell konvertiert. Die Konvertierung beinhaltet alle zuvor festgelegten Optimierungen und Quantisierungseinstellungen, um die Modellgröße zu reduzieren und die Ausführungsgeschwindigkeit zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dk100\\AppData\\Local\\Temp\\tmpr1fu5m91\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dk100\\AppData\\Local\\Temp\\tmpr1fu5m91\\assets\n",
      "C:\\Users\\dk100\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichern des konvertierten Modells als `.tflite-Datei`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if quantization:\n",
    "    if quant_dtype == \"int8 only\":\n",
    "        open(os.path.join(\"assets\", \"models\", \"tflite\", \"mnist_model_quantized_int8.tflite\"), \"wb\").write(tflite_model)\n",
    "    else:\n",
    "        open(os.path.join(\"assets\", \"models\", \"tflite\", \"mnist_model_quantized.tflite\"), \"wb\").write(tflite_model)\n",
    "else:\n",
    "    open(os.path.join(\"assets\", \"models\", \"tflite\", \"mnist_model.tflite\"), \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelkonvertierung zu C++-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Dateipfad des zu ladenden TensorFlow Lite-Modells wird definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if quantization:\n",
    "    if quant_dtype == \"int8 only\":\n",
    "        model_name = \"mnist_model_quantized_int8\"\n",
    "    else:\n",
    "        model_name = \"mnist_model_quantized\"\n",
    "else:\n",
    "    model_name = \"mnist_model\"\n",
    "tflite_model_path = os.path.join(\"assets\", \"models\", \"tflite\")\n",
    "cpp_model_path = os.path.join(\"assets\", \"models\", \"c++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die TensorFlow Lite Modelldatei wird im Binärmodus geöffnet. Der Inhalt der Datei wird in eine Hexadezimaldarstellung konvertiert. Anschließend wird der Hex-String zurück in ein ByteArray umgewandelt. Diese Umwandlung erleichtert die Formatierung des Modells, insbesondere wenn es in eine C++-Datei eingebettet werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Öffnen der TensorFlow Lite Modelldatei im Binärmodus\n",
    "with open(os.path.join(tflite_model_path, f\"{model_name}.tflite\"), 'rb') as f:\n",
    "    # Lesen des Inhalts der Datei und Konvertierung in eine Hexadezimaldarstellung\n",
    "    content = f.read().hex()\n",
    "    # Umwandlung des Hex-Strings zurück in ein ByteArray,\n",
    "    # um es leichter in einer C++-Datei zu formatieren.\n",
    "    result = bytearray.fromhex(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle wird eine neue C++-Quelldatei erstellt, um das TensorFlow Lite Modell als Datenarray einzubetten. Zu Beginn wird der notwendige C++-Header und die Deklaration des Datenarrays in die C++-Datei geschrieben.\n",
    "\n",
    "Die C++-Header umfassen Bedingungsprüfungen, die sicherstellen, dass das Datenarray auf verschiedenen Architekturen korrekt ausgerichtet ist. Anschließend wird das Modell in ein Datenarray geschrieben, wobei jedes Byte des Modells als Hexadezimalwert formatiert wird.\n",
    "\n",
    "Eine Schleife iteriert über jedes Byte im Modell und formatiert die Werte in Zeilen, um die Lesbarkeit zu verbessern. Abschließend wird die Deklaration des Arrays geschlossen und die Länge des Arrays als Konstante deklariert. Diese Datei kann dann in C++-Projekten verwendet werden, um das Modell direkt in den Code einzubetten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer neuen C++-Quelldatei für das Modell, um die Daten einzubetten\n",
    "with open(os.path.join(cpp_model_path, f\"{model_name}_data.cpp\"), \"wb\") as w:\n",
    "    values_in_row = 0\n",
    "    num_values = 0\n",
    "\n",
    "    # Schreiben des Headers der C++-Datei\n",
    "    w.write(bytearray('#include \"' + model_name + '_data.h\"\\n'\n",
    "                        \"\\n\"\n",
    "                        \"// We need to keep the data array aligned on\"\n",
    "                        \"some architectures.\\n\"\n",
    "                        \"#ifdef __has_attribute\\n\"\n",
    "                        \"#define HAVE_ATTRIBUTE(x) __has_attribute(x)\\n\"\n",
    "                        \"#else\\n\"\n",
    "                        \"#define HAVE_ATTRIBUTE(x) 0\\n\"\n",
    "                        \"#endif\\n\"\n",
    "                        \"#if HAVE_ATTRIBUTE(aligned) || (defined(\"\n",
    "                        \"__GNUC__) && !defined(__clang__))\\n\"\n",
    "                        \"#define DATA_ALIGN_ATTRIBUTE __attribute__(\"\n",
    "                        \"(aligned(4)))\\n\"\n",
    "                        \"#define DATA_ALIGN_ATTRIBUTE __attribute__(\"\n",
    "                        \"(aligned(4)))\\n\"\n",
    "                        \"#else\\n\"\n",
    "                        \"#define DATA_ALIGN_ATTRIBUTE\\n\"\n",
    "                        \"#endif\\n\"\n",
    "                        \"\\n\"\n",
    "\n",
    "                        \"const unsigned char \" + model_name + \"_tflite[]\"\n",
    "                        \" DATA_ALIGN_ATTRIBUTE = {\\n    \", 'utf-8'))\n",
    "    # Iterieren über jedes Byte in den Modelldaten und Formatierung jedes\n",
    "    # Bytes als Hexadezimalwert\n",
    "    for value in result:\n",
    "        num_values += 1\n",
    "        values_in_row += 1\n",
    "        value = \"0x{:02x}\".format(value)\n",
    "\n",
    "        # Logik zur Formatierung der Werte im Array\n",
    "        if values_in_row == 1:\n",
    "            w.write(bytearray(value, 'utf-8'))\n",
    "        elif values_in_row == 12:\n",
    "            w.write(bytearray(\", \" + str(value) + \",\\n    \", 'utf-8'))\n",
    "            values_in_row = 0\n",
    "        else:\n",
    "            w.write(bytearray(', ' + str(value), 'utf-8'))\n",
    "    # Abschluss der Arraydeklaration und der Datei\n",
    "    w.write(bytearray(\"};\\nconst int \" + model_name + \"_tflite_len = \"\n",
    "                        + str(num_values) + \";\", 'utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellen einer Header-Datei, die das Datenarray und dessen Länge definiert. Diese Header-Datei ist notwendig, um das Datenarray und seine Länge in anderen C++-Quelldateien zugänglich zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cpp_model_path, f\"{model_name}_data.h\"), \"w\") as f:\n",
    "    f.write('#ifndef TENSORFLOW_LITE_MODEL_DATA_H_\\n'\n",
    "            '#define TENSORFLOW_LITE_MODEL_DATA_H_\\n'\n",
    "            '\\n'\n",
    "            'extern const unsigned char ' + model_name + '_tflite[];\\n'\n",
    "            'extern const int ' + model_name + '_tflite_len;\\n'\n",
    "            '\\n'\n",
    "            '#endif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einsatz von TensorFlow Lite Modellen auf Raspberry Pi und anderen Einplatinencomputern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls das Modell mit `int8 only` quantisiert wurde, müssen die Testdaten in `uint8` umgewandelt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if quantization and \"int8 only\" in quant_dtype:\n",
    "    X_test_lite = (X_test * 255).astype(np.uint8)\n",
    "else:\n",
    "    X_test_lite = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesen der TensorFlow Lite Datei, in Abhängigkeit ob das Modell quantisiert wurde und falls ja welchen Datetyp die Eingabedaten haben müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if quantization:\n",
    "    if quant_dtype == \"int8 only\":\n",
    "        with open(os.path.join(\"assets\", \"models\", \"tflite\", \"mnist_model_quantized_int8.tflite\"), 'rb') as f:\n",
    "            tflite_model = f.read()\n",
    "    else:\n",
    "        with open(os.path.join(\"assets\", \"models\", \"tflite\", \"mnist_model_quantized.tflite\"), 'rb') as f:\n",
    "            tflite_model = f.read()\n",
    "else:\n",
    "    with open(os.path.join(\"assets\", \"models\", \"tflite\", \"mnist_model_quantized.tflite\"), 'rb') as f:\n",
    "        tflite_model = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird ein `tf.lite.Interpreter` erstellt und mit dem Inhalt des TensorFlow Lite-Modells initialisiert, um die Eingabe- und Ausgabedetails des Modells abzurufen und Speicher für die Ausführung des Modells zuzuweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des TensorFlow Lite Interpreters mit dem quantisierten Modell\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "# Allokieren von Speicher für den Interpreter, um das Modell auszuführen\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Abrufen der Details für die Eingabedaten des Modells\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "# Abrufen der Details für die Ausgabedaten des Modells\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Zelle wird die Form der Testdaten überprüft, um sicherzustellen, dass sie mit den Eingabeanforderungen des Modells übereinstimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_test_lite.shape) != 5:\n",
    "    X_test_lite = np.expand_dims(X_test_lite, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für alle Testdaten werden Vorhersagen getroffen und in der Liste `predictions` gesammelt.<br>\n",
    "Für jeden Datenpunkt werden folgende Schritte durchgeführt:\n",
    "1. Übergabe der Eingabedaten an den Eingabetensor des Modells mit der Methode `set_tensor`.\n",
    "2. Ausführung des Modells mit der Methode `invoke` aus.\n",
    "3. Erhalten der Vorhersage des Modells aus dem Ausgabetensor mit der Methode `get_tensor`.\n",
    "4. Vorhersage der Liste `predictions` hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sample in X_test_lite:\n",
    "    # Eingabedaten dem Eingabetensor des Modells übergeben\n",
    "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "    # Ausführen des Modells\n",
    "    interpreter.invoke()\n",
    "    # Vorhersage des Modells aus dem Ausgabetensor erhalten\n",
    "    pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # Aktuelle Vorhersage 'predictions' hinzufügen\n",
    "    predictions.append(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letztlich wird die Genauigkeit des TensorFlow Lite-Modells anhand der Testdaten (`X_test_lite` und `y_test`) bestimmt. Hierfür werden einzelnen Elemente der Liste der Vorhersagen (`predictions`) und die tatsächlichen Labels (`y_test`) miteinander verglichen und überprüft, ob diese übereinstimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite Modellgenauigkeit: 0.9763\n"
     ]
    }
   ],
   "source": [
    "model_acc = sum(1 for a,b in zip(predictions,y_test) if a == b) / len(predictions)\n",
    "print(f\"TensorFlow Lite Modellgenauigkeit: {model_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Code_test_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
